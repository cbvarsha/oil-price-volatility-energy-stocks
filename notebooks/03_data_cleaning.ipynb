{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1518e2cf-dfcf-475e-8e01-52bce4004d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XOM] Reading file: FDS project\\XOM_News_2019.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2020.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2021.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2022.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2023.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2024.csv\n",
      "[XOM] Reading file: FDS project\\XOM_News_2025.csv\n",
      "[XOM] Cleaned file saved to: FDS project\\XOM_News_cleaned.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2019.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2020.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2021.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2022.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2023.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2024.csv\n",
      "[Chevron] Reading file: FDS project\\Chevron_News_2025.csv\n",
      "[Chevron] Cleaned file saved to: FDS project\\Chevron_News_cleaned.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2019.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2020.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2021.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2022.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2023.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2024.csv\n",
      "[Shell] Reading file: FDS project\\Shell_News_2025.csv\n",
      "[Shell] Cleaned file saved to: FDS project\\Shell_News_cleaned.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2019.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2020.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2021.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2022.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2023.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2024.csv\n",
      "[BP] Reading file: FDS project\\BP_News_2025.csv\n",
      "[BP] Cleaned file saved to: FDS project\\BP_News_cleaned.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2019.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2020.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2021.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2022.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2023.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2024.csv\n",
      "[WTI] Reading file: FDS project\\WTI_News_2025.csv\n",
      "[WTI] Cleaned file saved to: FDS project\\WTI_News_cleaned.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2019.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2020.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2021.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2022.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2023.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2024.csv\n",
      "[Brent] Reading file: FDS project\\Brent_News_2025.csv\n",
      "[Brent] Cleaned file saved to: FDS project\\Brent_News_cleaned.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2019.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2020.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2021.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2022.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2023.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2024.csv\n",
      "[SPY] Reading file: FDS project\\SPY_News_2025.csv\n",
      "[SPY] Cleaned file saved to: FDS project\\SPY_News_cleaned.csv\n",
      "\n",
      "=== Summary Table ===\n",
      "    Entity  Initial  AfterMissing  AfterDup\n",
      "0      XOM     4217          4217      2663\n",
      "1  Chevron     4858          4858      3160\n",
      "2    Shell     5624          5624      4180\n",
      "3       BP     5246          5246      3740\n",
      "4      WTI     3982          3982      2428\n",
      "5    Brent     4257          4257      2442\n",
      "6      SPY     5050          5050      3438\n",
      "\n",
      "Summary table saved to: FDS project\\News_Consolidation_Summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "FDS_FOLDER = \"FDS project\"\n",
    "\n",
    "# 1) Define the CSV files for each entity\n",
    "#    Key = Entity name, Value = list of CSV filenames\n",
    "entities_files = {\n",
    "    \"XOM\": [\n",
    "        \"XOM_News_2019.csv\",\n",
    "        \"XOM_News_2020.csv\",\n",
    "        \"XOM_News_2021.csv\",\n",
    "        \"XOM_News_2022.csv\",\n",
    "        \"XOM_News_2023.csv\",\n",
    "        \"XOM_News_2024.csv\",\n",
    "        \"XOM_News_2025.csv\"\n",
    "    ],\n",
    "    \"Chevron\": [\n",
    "        \"Chevron_News_2019.csv\",\n",
    "        \"Chevron_News_2020.csv\",\n",
    "        \"Chevron_News_2021.csv\",\n",
    "        \"Chevron_News_2022.csv\",\n",
    "        \"Chevron_News_2023.csv\",\n",
    "        \"Chevron_News_2024.csv\",\n",
    "        \"Chevron_News_2025.csv\"\n",
    "    ],\n",
    "    \"Shell\": [\n",
    "        \"Shell_News_2019.csv\",\n",
    "        \"Shell_News_2020.csv\",\n",
    "        \"Shell_News_2021.csv\",\n",
    "        \"Shell_News_2022.csv\",\n",
    "        \"Shell_News_2023.csv\",\n",
    "        \"Shell_News_2024.csv\",\n",
    "        \"Shell_News_2025.csv\"\n",
    "    ],\n",
    "    \"BP\": [\n",
    "        \"BP_News_2019.csv\",\n",
    "        \"BP_News_2020.csv\",\n",
    "        \"BP_News_2021.csv\",\n",
    "        \"BP_News_2022.csv\",\n",
    "        \"BP_News_2023.csv\",\n",
    "        \"BP_News_2024.csv\",\n",
    "        \"BP_News_2025.csv\"\n",
    "    ],\n",
    "    \"WTI\": [\n",
    "        \"WTI_News_2019.csv\",\n",
    "        \"WTI_News_2020.csv\",\n",
    "        \"WTI_News_2021.csv\",\n",
    "        \"WTI_News_2022.csv\",\n",
    "        \"WTI_News_2023.csv\",\n",
    "        \"WTI_News_2024.csv\",\n",
    "        \"WTI_News_2025.csv\"\n",
    "    ],\n",
    "    \"Brent\": [\n",
    "        \"Brent_News_2019.csv\",\n",
    "        \"Brent_News_2020.csv\",\n",
    "        \"Brent_News_2021.csv\",\n",
    "        \"Brent_News_2022.csv\",\n",
    "        \"Brent_News_2023.csv\",\n",
    "        \"Brent_News_2024.csv\",\n",
    "        \"Brent_News_2025.csv\"\n",
    "    ],\n",
    "    \"SPY\": [\n",
    "        \"SPY_News_2019.csv\",\n",
    "        \"SPY_News_2020.csv\",\n",
    "        \"SPY_News_2021.csv\",\n",
    "        \"SPY_News_2022.csv\",\n",
    "        \"SPY_News_2023.csv\",\n",
    "        \"SPY_News_2024.csv\",\n",
    "        \"SPY_News_2025.csv\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2) A helper function to consolidate & clean files for one entity\n",
    "def consolidate_and_clean(entity, file_list):\n",
    "    \"\"\"\n",
    "    Reads all CSVs in file_list, concatenates them, removes missing & duplicates,\n",
    "    parses date strings, reformats to DD:MM:YYYY, and saves a cleaned file.\n",
    "    Returns stats in a dictionary.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    # Read each CSV for this entity\n",
    "    for filename in file_list:\n",
    "        csv_path = os.path.join(FDS_FOLDER, filename)\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f\"[{entity}] Reading file: {csv_path}\")\n",
    "            df_temp = pd.read_csv(csv_path)\n",
    "            all_dfs.append(df_temp)\n",
    "        else:\n",
    "            print(f\"[{entity}] File not found: {csv_path} (skipping)\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        # No files found for this entity\n",
    "        return {\n",
    "            \"Entity\": entity,\n",
    "            \"Initial\": 0,\n",
    "            \"AfterMissing\": 0,\n",
    "            \"AfterDup\": 0\n",
    "        }\n",
    "\n",
    "    # Concatenate\n",
    "    df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    initial_count = len(df_combined)\n",
    "\n",
    "    # Drop rows missing crucial columns (title, summary, link)\n",
    "    df_combined.dropna(subset=[\"title\", \"summary\", \"link\"], inplace=True)\n",
    "    after_missing_count = len(df_combined)\n",
    "\n",
    "    # Remove duplicates by (title, link)\n",
    "    dup_count = df_combined.duplicated(subset=[\"title\", \"link\"]).sum()\n",
    "    if dup_count > 0:\n",
    "        df_combined.drop_duplicates(subset=[\"title\", \"link\"], inplace=True)\n",
    "    after_dup_count = len(df_combined)\n",
    "\n",
    "    # --- NEW PART: Parse and reformat the date column ---\n",
    "    # We'll assume the column is named \"published_date\".\n",
    "    # 1) Convert to datetime\n",
    "    df_combined[\"published_date\"] = pd.to_datetime(\n",
    "        df_combined[\"published_date\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    # 2) Reformat to DD:MM:YYYY\n",
    "    #    (This will produce strings like \"20:03:2020\")\n",
    "    df_combined[\"published_date\"] = df_combined[\"published_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_filename = f\"{entity}_News_cleaned.csv\"\n",
    "    output_path = os.path.join(FDS_FOLDER, output_filename)\n",
    "    df_combined.to_csv(output_path, index=False)\n",
    "    print(f\"[{entity}] Cleaned file saved to: {output_path}\")\n",
    "    \n",
    "    # Return stats\n",
    "    return {\n",
    "        \"Entity\": entity,\n",
    "        \"Initial\": initial_count,\n",
    "        \"AfterMissing\": after_missing_count,\n",
    "        \"AfterDup\": after_dup_count\n",
    "    }\n",
    "\n",
    "# 3) Consolidate and clean for each entity, collect stats\n",
    "stats_list = []\n",
    "for entity, file_list in entities_files.items():\n",
    "    stats = consolidate_and_clean(entity, file_list)\n",
    "    stats_list.append(stats)\n",
    "\n",
    "# 4) Create a summary table (DataFrame) of the stats\n",
    "df_stats = pd.DataFrame(stats_list, columns=[\"Entity\", \"Initial\", \"AfterMissing\", \"AfterDup\"])\n",
    "print(\"\\n=== Summary Table ===\")\n",
    "print(df_stats)\n",
    "\n",
    "# 5) (Optional) Save the summary table to a CSV\n",
    "summary_output = os.path.join(FDS_FOLDER, \"News_Consolidation_Summary.csv\")\n",
    "df_stats.to_csv(summary_output, index=False)\n",
    "print(f\"\\nSummary table saved to: {summary_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9449fc59-4b65-4c75-820e-b63d03b929f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FDS project\\XOM_data.csv...\n",
      "Renamed columns to: ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Saved cleaned file to: FDS project\\XOM_data_cleaned.csv\n",
      "\n",
      "Processing FDS project\\CVX_data.csv...\n",
      "Renamed columns to: ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Saved cleaned file to: FDS project\\CVX_data_cleaned.csv\n",
      "\n",
      "Processing FDS project\\BP_data.csv...\n",
      "Renamed columns to: ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Saved cleaned file to: FDS project\\BP_data_cleaned.csv\n",
      "\n",
      "Processing FDS project\\SHEL_data.csv...\n",
      "Renamed columns to: ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Saved cleaned file to: FDS project\\SHEL_data_cleaned.csv\n",
      "\n",
      "Processing FDS project\\SPY_data.csv...\n",
      "Renamed columns to: ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
      "Saved cleaned file to: FDS project\\SPY_data_cleaned.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder with your CSV files\n",
    "FDS_FOLDER = \"FDS project\"\n",
    "\n",
    "# CSV files to process\n",
    "csv_files = [\n",
    "    \"XOM_data.csv\",\n",
    "    \"CVX_data.csv\",\n",
    "    \"BP_data.csv\",\n",
    "    \"SHEL_data.csv\",\n",
    "    \"SPY_data.csv\"\n",
    "]\n",
    "\n",
    "# Final column names in the exact order we want\n",
    "final_columns = [\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
    "\n",
    "for filename in csv_files:\n",
    "    input_path = os.path.join(FDS_FOLDER, filename)\n",
    "    print(f\"Processing {input_path}...\")\n",
    "\n",
    "    # 1) Read CSV, skip first 2 lines (unwanted headers)\n",
    "    #    so line 3 becomes data\n",
    "    # 2) Use header=None so we can rename columns ourselves\n",
    "    df = pd.read_csv(input_path, skiprows=2, header=None)\n",
    "\n",
    "    # 3) Force the columns to the final names\n",
    "    #    Make sure the CSV after skipping lines has exactly 6 columns\n",
    "    df.columns = final_columns\n",
    "\n",
    "    # 4) Build output filename (e.g. \"XOM_data_cleaned.csv\")\n",
    "    output_filename = filename.replace(\".csv\", \"_cleaned.csv\")\n",
    "    output_path = os.path.join(FDS_FOLDER, output_filename)\n",
    "\n",
    "    # 5) Save the cleaned CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Renamed columns to: {final_columns}\")\n",
    "    print(f\"Saved cleaned file to: {output_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901f910-092b-498b-a1ea-587098adbcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
